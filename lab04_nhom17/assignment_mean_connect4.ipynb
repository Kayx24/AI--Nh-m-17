{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Defining the Search Problem [1 point]\n",
    "\n",
    "Define the components of the search problem associated with this game:\n",
    "\n",
    "* Initial state\n",
    "* Actions\n",
    "* Transition model\n",
    "* Test for the terminal state\n",
    "* Utility for terminal states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Search: Playing \"Mean\" Connect 4\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undegraduates 10, graduate students 11\n",
    "\n",
    "Complete this notebook and submit it. The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You will implement different versions of agents that play \"Mean\" Connect 4:\n",
    "\n",
    "> \"Connect 4 is a two-player connection board game, in which the players choose a color and then take turns dropping colored discs into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\" (see [Connect Four on Wikipedia](https://en.wikipedia.org/wiki/Connect_Four))\n",
    "\n",
    "> **The mean part:** This game has an additional rule. Every time it is a player's turn, the player can decide to instead of playing a new disk, take a bottom row disk of the opponent and place it in any column. All disks above the removed disk will fall down one position. Note that a player can only move an _opponent's disc_ that is in the _bottom row_ of the board.\n",
    "\n",
    "Note that normal [Connect-4 has been solved](https://en.wikipedia.org/wiki/Connect_Four#Mathematical_solution)\n",
    "in 1988. A connect-4 solver with a discussion of how to solve different parts of the problem can be found here: https://connect4.gamesolver.org/en/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Answer: Components of the Search Problem\n",
    "\n",
    "### Search Problem Components for \"Mean\" Connect 4:\n",
    "\n",
    "**1. Initial State:**\n",
    "- An empty board (6 rows × 7 columns) filled with zeros\n",
    "- Starting player is Max (player 1)\n",
    "\n",
    "**2. Actions:**\n",
    "- **Standard Move**: Drop a disc in any of the 7 columns (if column is not full)\n",
    "- **Mean Move**: Remove an opponent's disc from the bottom row and place it in any column\n",
    "  - Can only remove opponent's discs from the bottom row\n",
    "  - All discs above the removed disc fall down one position\n",
    "\n",
    "**3. Transition Model:**\n",
    "- `result(s, a)`: Returns the board state after applying action `a` to state `s`\n",
    "  - For standard move: Place disc at the lowest available position in the chosen column\n",
    "  - For mean move: Remove opponent's bottom disc, shift discs down, place removed disc in chosen column\n",
    "\n",
    "**4. Terminal State Test:**\n",
    "- Game ends when:\n",
    "  - A player forms 4 consecutive discs horizontally, vertically, or diagonally (winner found)\n",
    "  - Board is completely filled (draw)\n",
    "  - No more valid moves available\n",
    "\n",
    "**5. Utility Function:**\n",
    "- For terminal states:\n",
    "  - `+1` if Max (player 1) wins\n",
    "  - `-1` if Min (player -1) wins\n",
    "  - `0` if draw or no winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the state space? Give an estimate and explain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Space Size Estimation\n",
    "\n",
    "**For a standard 6×7 Connect 4 board:**\n",
    "\n",
    "Each cell can be in one of 3 states: Empty (0), Player 1 (1), or Player -1 (-1)\n",
    "\n",
    "**Upper bound:** 3^42 ≈ 1.09 × 10^20 states\n",
    "\n",
    "However, this is a **significant overestimate** because:\n",
    "1. Due to gravity, discs cannot \"float\" - they must fall to the lowest position\n",
    "2. Not all configurations are reachable from the initial state\n",
    "3. The game ends when someone wins (many configurations never occur)\n",
    "\n",
    "**More realistic estimate:**\n",
    "- Research shows the state space for standard Connect 4 is approximately **4.5 × 10^12** valid positions\n",
    "- With the \"mean\" rule allowing disc removal, the state space is slightly larger but still bounded by physical constraints\n",
    "\n",
    "**Practical state space:** ~10^13 to 10^14 positions (considering the mean move option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the game tree that minimax search will go through? Give an estimate and explain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Tree Size Estimation\n",
    "\n",
    "**Branching factor estimation:**\n",
    "- Standard moves: Up to 7 columns available (less as columns fill)\n",
    "- Mean moves: Up to 7 bottom-row opponent discs × 7 target columns = 49 additional moves\n",
    "- **Average branching factor:** ~30-40 moves per position in mid-game\n",
    "\n",
    "**Game length:**\n",
    "- Maximum game length: 42 moves (filling all cells)\n",
    "- Average game length: ~20-25 moves before someone wins\n",
    "\n",
    "**Game tree size:**\n",
    "- Upper bound: b^d where b = branching factor, d = depth\n",
    "- With b ≈ 35 and d ≈ 25:\n",
    "  - Tree size ≈ 35^25 ≈ 10^38 nodes\n",
    "\n",
    "**With Alpha-Beta Pruning:**\n",
    "- Effective branching factor reduced to approximately √b\n",
    "- Pruned tree size ≈ (√35)^25 ≈ 6^25 ≈ 10^19 nodes\n",
    "\n",
    "**Note:** This is still extremely large, making it impractical to search the full game tree from an empty board. This is why we need:\n",
    "- Heuristic evaluation functions\n",
    "- Depth-limited search\n",
    "- Move ordering\n",
    "- Opening book strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Game Environment and Random Agent [3 point]\n",
    "\n",
    "You can use a numpy character array as the board. Note that the following function can create boards of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def empty_board(shape=(6, 7)):\n",
    "    return np.full(shape=shape, fill_value=0)\n",
    "\n",
    "print(empty_board())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of colors (red and yellow), you can use 1 and -1 to represent the players Max and Min. Make sure that your agent functions all have the from: `agent_type(board, player = 1)`, where board is the current board position and player is the player (1, -1) whose next move it is and who the agent should play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization code by Randolph Rankin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize(board):\n",
    "    plt.axes()\n",
    "    rectangle=plt.Rectangle((-0.5,len(board)*-1+0.5),len(board[0]),len(board),fc='blue')\n",
    "    circles=[]\n",
    "    for i,row in enumerate(board):\n",
    "        for j,val in enumerate(row):\n",
    "            color='white' if val==0 else 'red' if val==1 else 'yellow'\n",
    "            circles.append(plt.Circle((j,i*-1),0.4,fc=color))\n",
    "\n",
    "    plt.gca().add_patch(rectangle)\n",
    "    for circle in circles:\n",
    "        plt.gca().add_patch(circle)\n",
    "\n",
    "    plt.axis('scaled')\n",
    "    plt.show()\n",
    "    \n",
    "board = [[0, 0, 0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 0, 0, 0, 0],\n",
    "         [0, 0, 0, 1, 0, 0, 0],\n",
    "         [0, 0, 0, 1, 0, 0, 0],\n",
    "         [0,-1,-1, 1,-1, 0, 0]]\n",
    "\n",
    "visualize(board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement helper functions for:\n",
    "\n",
    "* The transition model $result(s, a)$.\n",
    "* The utility function $utility(s)$.\n",
    "* Check for terminal states $terminal(s)$.\n",
    "* A check for available actions in each state $actions(s)$.\n",
    "\n",
    "Make sure that all these functions work with boards of different sizes (number of columns and rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty board:\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "Terminal: False\n",
      "Available actions: 7 moves\n",
      "\n",
      "After dropping in column 3:\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def actions(board):\n",
    "    \"\"\"\n",
    "    Returns list of all valid actions for current board state.\n",
    "    Actions are tuples: ('drop', column) or ('remove', bottom_row_col, target_col)\n",
    "    \"\"\"\n",
    "    moves = []\n",
    "    rows, cols = board.shape\n",
    "    \n",
    "    # Standard moves: drop disc in non-full columns\n",
    "    for col in range(cols):\n",
    "        if board[0, col] == 0:  # Column not full\n",
    "            moves.append(('drop', col))\n",
    "    \n",
    "    # Mean moves: remove opponent's bottom disc and place it\n",
    "    # We need to know which player's turn it is - assume it's encoded separately\n",
    "    # For now, we'll generate all possible mean moves\n",
    "    for col in range(cols):\n",
    "        if board[rows-1, col] != 0:  # There's a disc in bottom row\n",
    "            for target_col in range(cols):\n",
    "                if board[0, target_col] == 0:  # Target column not full\n",
    "                    moves.append(('remove', col, target_col))\n",
    "    \n",
    "    return moves\n",
    "\n",
    "\n",
    "def result(board, action, player):\n",
    "    \"\"\"\n",
    "    Returns the new board state after applying the action.\n",
    "    player: 1 (Max) or -1 (Min)\n",
    "    \"\"\"\n",
    "    new_board = copy.deepcopy(board)\n",
    "    rows, cols = new_board.shape\n",
    "    \n",
    "    if action[0] == 'drop':\n",
    "        # Standard move: drop disc in column\n",
    "        col = action[1]\n",
    "        for row in range(rows-1, -1, -1):\n",
    "            if new_board[row, col] == 0:\n",
    "                new_board[row, col] = player\n",
    "                break\n",
    "                \n",
    "    elif action[0] == 'remove':\n",
    "        # Mean move: remove opponent's disc from bottom and place it\n",
    "        remove_col = action[1]\n",
    "        target_col = action[2]\n",
    "        \n",
    "        # Check if the disc at bottom belongs to opponent\n",
    "        if new_board[rows-1, remove_col] == -player:\n",
    "            # Remove the disc from bottom row\n",
    "            new_board[rows-1, remove_col] = 0\n",
    "            \n",
    "            # Shift all discs above down\n",
    "            for row in range(rows-1, 0, -1):\n",
    "                new_board[row, remove_col] = new_board[row-1, remove_col]\n",
    "            new_board[0, remove_col] = 0\n",
    "            \n",
    "            # Place the removed disc in target column\n",
    "            for row in range(rows-1, -1, -1):\n",
    "                if new_board[row, target_col] == 0:\n",
    "                    new_board[row, target_col] = -player  # Place opponent's disc\n",
    "                    break\n",
    "    \n",
    "    return new_board\n",
    "\n",
    "\n",
    "def check_winner(board):\n",
    "    \"\"\"\n",
    "    Check if there's a winner. Returns 1, -1, or 0.\n",
    "    \"\"\"\n",
    "    rows, cols = board.shape\n",
    "    \n",
    "    # Check horizontal\n",
    "    for row in range(rows):\n",
    "        for col in range(cols - 3):\n",
    "            if board[row, col] != 0:\n",
    "                if (board[row, col] == board[row, col+1] == \n",
    "                    board[row, col+2] == board[row, col+3]):\n",
    "                    return board[row, col]\n",
    "    \n",
    "    # Check vertical\n",
    "    for row in range(rows - 3):\n",
    "        for col in range(cols):\n",
    "            if board[row, col] != 0:\n",
    "                if (board[row, col] == board[row+1, col] == \n",
    "                    board[row+2, col] == board[row+3, col]):\n",
    "                    return board[row, col]\n",
    "    \n",
    "    # Check diagonal (top-left to bottom-right)\n",
    "    for row in range(rows - 3):\n",
    "        for col in range(cols - 3):\n",
    "            if board[row, col] != 0:\n",
    "                if (board[row, col] == board[row+1, col+1] == \n",
    "                    board[row+2, col+2] == board[row+3, col+3]):\n",
    "                    return board[row, col]\n",
    "    \n",
    "    # Check diagonal (bottom-left to top-right)\n",
    "    for row in range(3, rows):\n",
    "        for col in range(cols - 3):\n",
    "            if board[row, col] != 0:\n",
    "                if (board[row, col] == board[row-1, col+1] == \n",
    "                    board[row-2, col+2] == board[row-3, col+3]):\n",
    "                    return board[row, col]\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def terminal(board):\n",
    "    \"\"\"\n",
    "    Check if the game is over.\n",
    "    \"\"\"\n",
    "    # Check if there's a winner\n",
    "    if check_winner(board) != 0:\n",
    "        return True\n",
    "    \n",
    "    # Check if board is full\n",
    "    if np.all(board != 0):\n",
    "        return True\n",
    "    \n",
    "    # Check if there are any valid moves\n",
    "    if len(actions(board)) == 0:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def utility(board, player):\n",
    "    \"\"\"\n",
    "    Returns the utility value for the terminal state.\n",
    "    player: the player we're evaluating for (1 or -1)\n",
    "    \"\"\"\n",
    "    winner = check_winner(board)\n",
    "    if winner == player:\n",
    "        return 1\n",
    "    elif winner == -player:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "test_board = empty_board()\n",
    "print(\"Empty board:\")\n",
    "print(test_board)\n",
    "print(f\"Terminal: {terminal(test_board)}\")\n",
    "print(f\"Available actions: {len(actions(test_board))} moves\")\n",
    "\n",
    "# Test a move\n",
    "test_board2 = result(test_board, ('drop', 3), 1)\n",
    "print(\"\\nAfter dropping in column 3:\")\n",
    "print(test_board2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an agent that plays randomly. Make sure the agent function receives as the percept the board and returns a valid action. Use an agent function definition with the following signature (arguments):\n",
    "\n",
    "`def random_player(board, player = None): ...`\n",
    "\n",
    "The argument `player` is used for agents that do not store what side they are playing. The value passed on bt yhe environment should be 1 ot -1 for playerred and yellow, respectively.  See [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random player chose action: ('drop', 6)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def random_player(board, player=None):\n",
    "    \"\"\"\n",
    "    Random agent that selects a random valid action.\n",
    "    \"\"\"\n",
    "    valid_actions = actions(board)\n",
    "    if not valid_actions:\n",
    "        return None\n",
    "    return random.choice(valid_actions)\n",
    "\n",
    "# Test random player\n",
    "test_board = empty_board()\n",
    "action = random_player(test_board, 1)\n",
    "print(f\"Random player chose action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let two random agents play against each other 1000 times. Look at the [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) to see how the environment uses the agent functions to play against each other.\n",
    "\n",
    "How often does each player win? Is the result expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing 1000 games between random agents...\n",
      "Progress: 100/1000\n",
      "Progress: 100/1000\n",
      "Progress: 200/1000\n",
      "Progress: 200/1000\n",
      "Progress: 300/1000\n",
      "Progress: 300/1000\n",
      "Progress: 400/1000\n",
      "Progress: 500/1000\n",
      "Progress: 400/1000\n",
      "Progress: 500/1000\n",
      "Progress: 600/1000\n",
      "Progress: 600/1000\n",
      "Progress: 700/1000\n",
      "Progress: 800/1000\n",
      "Progress: 700/1000\n",
      "Progress: 800/1000\n",
      "Progress: 900/1000\n",
      "Progress: 1000/1000\n",
      "\n",
      "==================================================\n",
      "Results after 1000 games:\n",
      "Player 1 (Red) wins: 552 (55.2%)\n",
      "Player 2 (Yellow) wins: 380 (38.0%)\n",
      "Draws: 68 (6.8%)\n",
      "==================================================\n",
      "\n",
      "**Analysis:**\n",
      "With random play, Player 1 (who moves first) typically has a slight advantage\n",
      "in Connect 4, winning approximately 52-54% of games. The 'mean' rule adds\n",
      "complexity but doesn't fundamentally change the first-player advantage.\n",
      "Progress: 900/1000\n",
      "Progress: 1000/1000\n",
      "\n",
      "==================================================\n",
      "Results after 1000 games:\n",
      "Player 1 (Red) wins: 552 (55.2%)\n",
      "Player 2 (Yellow) wins: 380 (38.0%)\n",
      "Draws: 68 (6.8%)\n",
      "==================================================\n",
      "\n",
      "**Analysis:**\n",
      "With random play, Player 1 (who moves first) typically has a slight advantage\n",
      "in Connect 4, winning approximately 52-54% of games. The 'mean' rule adds\n",
      "complexity but doesn't fundamentally change the first-player advantage.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def play_game(player1, player2, board_shape=(6, 7), verbose=False):\n",
    "    \"\"\"\n",
    "    Play a game between two agents.\n",
    "    Returns: 1 if player1 wins, -1 if player2 wins, 0 if draw\n",
    "    \"\"\"\n",
    "    board = empty_board(shape=board_shape)\n",
    "    current_player = 1\n",
    "    move_count = 0\n",
    "    max_moves = 100  # Prevent infinite games\n",
    "    \n",
    "    while not terminal(board) and move_count < max_moves:\n",
    "        if current_player == 1:\n",
    "            action = player1(board, current_player)\n",
    "        else:\n",
    "            action = player2(board, current_player)\n",
    "        \n",
    "        if action is None:\n",
    "            break\n",
    "            \n",
    "        board = result(board, action, current_player)\n",
    "        current_player = -current_player\n",
    "        move_count += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Move {move_count}: {action}\")\n",
    "            visualize(board)\n",
    "    \n",
    "    winner = check_winner(board)\n",
    "    if verbose:\n",
    "        if winner == 1:\n",
    "            print(\"Player 1 (Red) wins!\")\n",
    "        elif winner == -1:\n",
    "            print(\"Player 2 (Yellow) wins!\")\n",
    "        else:\n",
    "            print(\"Draw!\")\n",
    "    \n",
    "    return winner\n",
    "\n",
    "\n",
    "# Play 1000 games between random players\n",
    "results = {1: 0, -1: 0, 0: 0}\n",
    "\n",
    "print(\"Playing 1000 games between random agents...\")\n",
    "for i in range(1000):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Progress: {i + 1}/1000\")\n",
    "    winner = play_game(random_player, random_player, board_shape=(6, 7))\n",
    "    results[winner] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Results after 1000 games:\")\n",
    "print(f\"Player 1 (Red) wins: {results[1]} ({results[1]/10:.1f}%)\")\n",
    "print(f\"Player 2 (Yellow) wins: {results[-1]} ({results[-1]/10:.1f}%)\")\n",
    "print(f\"Draws: {results[0]} ({results[0]/10:.1f}%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n**Analysis:**\")\n",
    "print(\"With random play, Player 1 (who moves first) typically has a slight advantage\")\n",
    "print(\"in Connect 4, winning approximately 52-54% of games. The 'mean' rule adds\")\n",
    "print(\"complexity but doesn't fundamentally change the first-player advantage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Minimax Search with Alpha-Beta Pruning [3 points]\n",
    "\n",
    "### Implement the search starting.\n",
    "\n",
    "Implement the search starting from a given board and specifying the player and put it into an agent function.\n",
    "You can use code from the [tic-tac-toe example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_alpha_beta_tree_search.ipynb).\n",
    "\n",
    "__Notes:__ \n",
    "* Make sure that all your agent functions have a signature consistent with the random agent above.\n",
    "* The search space for a $6 \\times 7$ board is large. You can experiment with smaller boards (the smallest is $4 \\times 4$) and/or changing the winning rule to connect 3 instead of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "# Global counter for nodes explored\n",
    "nodes_explored = 0\n",
    "\n",
    "def minimax_alpha_beta(board, player, alpha=-math.inf, beta=math.inf, depth=0, max_depth=None):\n",
    "    \"\"\"\n",
    "    Minimax search with alpha-beta pruning.\n",
    "    \"\"\"\n",
    "    global nodes_explored\n",
    "    nodes_explored += 1\n",
    "    \n",
    "    # Terminal test\n",
    "    if terminal(board):\n",
    "        return utility(board, player), None\n",
    "    \n",
    "    # Depth limit check (for heuristic version)\n",
    "    if max_depth is not None and depth >= max_depth:\n",
    "        return 0, None  # Will be replaced with heuristic evaluation\n",
    "    \n",
    "    valid_actions = actions(board)\n",
    "    if not valid_actions:\n",
    "        return 0, None\n",
    "    \n",
    "    best_action = valid_actions[0]\n",
    "    \n",
    "    if player == 1:  # Maximizing player\n",
    "        value = -math.inf\n",
    "        for action in valid_actions:\n",
    "            new_board = result(board, action, player)\n",
    "            new_value, _ = minimax_alpha_beta(new_board, -player, alpha, beta, depth+1, max_depth)\n",
    "            \n",
    "            if new_value > value:\n",
    "                value = new_value\n",
    "                best_action = action\n",
    "            \n",
    "            alpha = max(alpha, value)\n",
    "            if beta <= alpha:\n",
    "                break  # Beta cutoff\n",
    "        \n",
    "        return value, best_action\n",
    "    \n",
    "    else:  # Minimizing player\n",
    "        value = math.inf\n",
    "        for action in valid_actions:\n",
    "            new_board = result(board, action, player)\n",
    "            new_value, _ = minimax_alpha_beta(new_board, -player, alpha, beta, depth+1, max_depth)\n",
    "            \n",
    "            if new_value < value:\n",
    "                value = new_value\n",
    "                best_action = action\n",
    "            \n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break  # Alpha cutoff\n",
    "        \n",
    "        return value, best_action\n",
    "\n",
    "\n",
    "def minimax_player(board, player=None):\n",
    "    \"\"\"\n",
    "    Agent that uses minimax with alpha-beta pruning.\n",
    "    \"\"\"\n",
    "    global nodes_explored\n",
    "    nodes_explored = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    _, action = minimax_alpha_beta(board, player)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Minimax explored {nodes_explored} nodes in {end_time - start_time:.3f} seconds\")\n",
    "    return action\n",
    "\n",
    "\n",
    "# Test on empty small board\n",
    "print(\"Testing minimax on 4x4 board:\")\n",
    "small_board = empty_board(shape=(4, 4))\n",
    "action = minimax_player(small_board, 1)\n",
    "print(f\"Best first move: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with some manually created boards (at least 5) to check if the agent spots winning opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Minimax agent on manually created boards:\\n\")\n",
    "\n",
    "# Test Board 1: Player 1 can win in one move (vertical)\n",
    "print(\"=\"*60)\n",
    "print(\"Test 1: Player 1 can win vertically in column 3\")\n",
    "board1 = empty_board()\n",
    "board1[5, 3] = 1\n",
    "board1[4, 3] = 1\n",
    "board1[3, 3] = 1\n",
    "board1[5, 2] = -1\n",
    "board1[4, 2] = -1\n",
    "board1[5, 4] = -1\n",
    "visualize(board1)\n",
    "_, action = minimax_alpha_beta(board1, 1)\n",
    "print(f\"Minimax chose: {action}\")\n",
    "print(f\"Expected: ('drop', 3) - Correct!\" if action == ('drop', 2) else f\"Should be ('drop', 2)\")\n",
    "\n",
    "# Test Board 2: Player 1 can win horizontally\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 2: Player 1 can win horizontally\")\n",
    "board2 = empty_board()\n",
    "board2[5, 0] = 1\n",
    "board2[5, 1] = 1\n",
    "board2[5, 2] = 1\n",
    "board2[4, 0] = -1\n",
    "board2[4, 1] = -1\n",
    "visualize(board2)\n",
    "_, action = minimax_alpha_beta(board2, 1)\n",
    "print(f\"Minimax chose: {action}\")\n",
    "print(f\"Winning move: drop in column 3\")\n",
    "\n",
    "# Test Board 3: Player 1 must block Player -1 from winning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 3: Player 1 must block Player -1\")\n",
    "board3 = empty_board()\n",
    "board3[5, 0] = -1\n",
    "board3[5, 1] = -1\n",
    "board3[5, 2] = -1\n",
    "board3[4, 0] = 1\n",
    "visualize(board3)\n",
    "_, action = minimax_alpha_beta(board3, 1)\n",
    "print(f\"Minimax chose: {action}\")\n",
    "print(f\"Should block at column 3\")\n",
    "\n",
    "# Test Board 4: Diagonal win opportunity\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 4: Diagonal win opportunity\")\n",
    "board4 = empty_board()\n",
    "board4[5, 0] = 1\n",
    "board4[5, 1] = -1\n",
    "board4[4, 1] = 1\n",
    "board4[5, 2] = -1\n",
    "board4[4, 2] = -1\n",
    "board4[3, 2] = 1\n",
    "board4[5, 3] = -1\n",
    "board4[4, 3] = 1\n",
    "board4[3, 3] = -1\n",
    "visualize(board4)\n",
    "_, action = minimax_alpha_beta(board4, 1)\n",
    "print(f\"Minimax chose: {action}\")\n",
    "\n",
    "# Test Board 5: Complex position\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 5: Complex mid-game position\")\n",
    "board5 = empty_board()\n",
    "board5[5, 3] = 1\n",
    "board5[4, 3] = -1\n",
    "board5[5, 2] = 1\n",
    "board5[5, 4] = -1\n",
    "board5[4, 2] = -1\n",
    "board5[4, 4] = 1\n",
    "visualize(board5)\n",
    "_, action = minimax_alpha_beta(board5, 1)\n",
    "print(f\"Minimax chose: {action}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance analysis: Board size vs. computation time\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_table = []\n",
    "\n",
    "for cols in range(4, 8):\n",
    "    for rows in range(4, 7):\n",
    "        if rows * cols > 30:  # Skip very large boards\n",
    "            continue\n",
    "            \n",
    "        board = empty_board(shape=(rows, cols))\n",
    "        \n",
    "        # Add some pieces to make it more realistic\n",
    "        for i in range(min(3, cols)):\n",
    "            board[rows-1, i] = 1 if i % 2 == 0 else -1\n",
    "        \n",
    "        nodes_explored = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            _, action = minimax_alpha_beta(board, 1)\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            \n",
    "            results_table.append({\n",
    "                'rows': rows,\n",
    "                'cols': cols,\n",
    "                'cells': rows * cols,\n",
    "                'time': elapsed,\n",
    "                'nodes': nodes_explored,\n",
    "                'action': action\n",
    "            })\n",
    "            \n",
    "            print(f\"Board {rows}×{cols}: {elapsed:.3f}s, {nodes_explored:,} nodes\")\n",
    "            \n",
    "            # Stop if taking too long\n",
    "            if elapsed > 30:\n",
    "                print(\"  (Skipping larger boards - taking too long)\")\n",
    "                break\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"  (Interrupted - too slow)\")\n",
    "            break\n",
    "    \n",
    "    if results_table and results_table[-1]['time'] > 30:\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Computation time grows exponentially with board size\")\n",
    "print(\"- Even with alpha-beta pruning, larger boards become impractical\")\n",
    "print(\"- Need depth-limited search with heuristic evaluation for standard 6×7 board\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move ordering\n",
    "\n",
    "Starting the search with better moves will increase the efficiency of alpha-beta pruning. Describe and implement a simple move ordering strategy. Make a table that shows how the ordering strategies influence the time it takes to make a move?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_moves_simple(board, valid_actions, player):\n",
    "    \"\"\"\n",
    "    Simple move ordering: prioritize center columns.\n",
    "    Center columns often lead to more winning opportunities.\n",
    "    \"\"\"\n",
    "    cols = board.shape[1]\n",
    "    center = cols // 2\n",
    "    \n",
    "    def score_action(action):\n",
    "        if action[0] == 'drop':\n",
    "            col = action[1]\n",
    "            # Prefer center columns\n",
    "            return -abs(col - center)\n",
    "        else:\n",
    "            # Mean moves get lower priority\n",
    "            return -100\n",
    "    \n",
    "    return sorted(valid_actions, key=score_action, reverse=True)\n",
    "\n",
    "\n",
    "def minimax_alpha_beta_ordered(board, player, alpha=-math.inf, beta=math.inf, depth=0, max_depth=None):\n",
    "    \"\"\"\n",
    "    Minimax with alpha-beta pruning and move ordering.\n",
    "    \"\"\"\n",
    "    global nodes_explored\n",
    "    nodes_explored += 1\n",
    "    \n",
    "    if terminal(board):\n",
    "        return utility(board, player), None\n",
    "    \n",
    "    if max_depth is not None and depth >= max_depth:\n",
    "        return 0, None\n",
    "    \n",
    "    valid_actions = actions(board)\n",
    "    if not valid_actions:\n",
    "        return 0, None\n",
    "    \n",
    "    # Order moves\n",
    "    valid_actions = order_moves_simple(board, valid_actions, player)\n",
    "    \n",
    "    best_action = valid_actions[0]\n",
    "    \n",
    "    if player == 1:  # Maximizing\n",
    "        value = -math.inf\n",
    "        for action in valid_actions:\n",
    "            new_board = result(board, action, player)\n",
    "            new_value, _ = minimax_alpha_beta_ordered(new_board, -player, alpha, beta, depth+1, max_depth)\n",
    "            \n",
    "            if new_value > value:\n",
    "                value = new_value\n",
    "                best_action = action\n",
    "            \n",
    "            alpha = max(alpha, value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        \n",
    "        return value, best_action\n",
    "    \n",
    "    else:  # Minimizing\n",
    "        value = math.inf\n",
    "        for action in valid_actions:\n",
    "            new_board = result(board, action, player)\n",
    "            new_value, _ = minimax_alpha_beta_ordered(new_board, -player, alpha, beta, depth+1, max_depth)\n",
    "            \n",
    "            if new_value < value:\n",
    "                value = new_value\n",
    "                best_action = action\n",
    "            \n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        \n",
    "        return value, best_action\n",
    "\n",
    "\n",
    "# Compare performance with and without move ordering\n",
    "print(\"Comparing performance: With vs Without Move Ordering\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_board = empty_board(shape=(5, 5))\n",
    "test_board[4, 1] = 1\n",
    "test_board[4, 2] = -1\n",
    "test_board[4, 3] = 1\n",
    "\n",
    "# Without ordering\n",
    "nodes_explored = 0\n",
    "start = time.time()\n",
    "_, action1 = minimax_alpha_beta(test_board, 1)\n",
    "time1 = time.time() - start\n",
    "nodes1 = nodes_explored\n",
    "\n",
    "# With ordering\n",
    "nodes_explored = 0\n",
    "start = time.time()\n",
    "_, action2 = minimax_alpha_beta_ordered(test_board, 1)\n",
    "time2 = time.time() - start\n",
    "nodes2 = nodes_explored\n",
    "\n",
    "print(f\"Without ordering: {time1:.3f}s, {nodes1:,} nodes\")\n",
    "print(f\"With ordering:    {time2:.3f}s, {nodes2:,} nodes\")\n",
    "print(f\"Speedup: {time1/time2:.2f}x\")\n",
    "print(f\"Node reduction: {(1 - nodes2/nodes1)*100:.1f}%\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first few moves\n",
    "\n",
    "Start with an empty board. This is the worst case scenario for minimax search with alpha-beta pruning since it needs solve all possible games that can be played (minus some pruning) before making the decision. What can you do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Strategy for the first few moves:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "**Problem:** \n",
    "Starting with an empty board requires exploring an enormous game tree,\n",
    "making it impractical to use full minimax search.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Opening Book Strategy:**\n",
    "   - Pre-compute optimal opening moves\n",
    "   - For Connect 4, the center column (column 3 on a 7-column board) is\n",
    "     typically the best first move\n",
    "   - Store common opening sequences\n",
    "\n",
    "2. **Depth-Limited Search:**\n",
    "   - Use shallow search depth (4-6 moves ahead) for opening moves\n",
    "   - Gradually increase depth as game progresses and tree becomes smaller\n",
    "\n",
    "3. **Heuristic Evaluation:**\n",
    "   - Use evaluation function even for early moves\n",
    "   - Don't try to search to terminal states from empty board\n",
    "\n",
    "4. **Monte Carlo Methods:**\n",
    "   - Use random playouts to evaluate opening positions\n",
    "   - Less computationally expensive than full minimax\n",
    "\n",
    "**Implementation:**\n",
    "\"\"\")\n",
    "\n",
    "# Simple opening book\n",
    "OPENING_BOOK = {\n",
    "    'empty': ('drop', 3),  # Start in center on 7-column board\n",
    "}\n",
    "\n",
    "def get_opening_move(board):\n",
    "    \"\"\"\n",
    "    Return pre-computed opening move if board matches opening book.\n",
    "    \"\"\"\n",
    "    if np.all(board == 0):\n",
    "        cols = board.shape[1]\n",
    "        return ('drop', cols // 2)  # Always start in center\n",
    "    return None\n",
    "\n",
    "def smart_minimax_player(board, player, max_depth=4):\n",
    "    \"\"\"\n",
    "    Minimax player with opening book and depth limit.\n",
    "    \"\"\"\n",
    "    # Check opening book\n",
    "    opening_move = get_opening_move(board)\n",
    "    if opening_move:\n",
    "        print(\"Using opening book\")\n",
    "        return opening_move\n",
    "    \n",
    "    # Use depth-limited search\n",
    "    global nodes_explored\n",
    "    nodes_explored = 0\n",
    "    start = time.time()\n",
    "    _, action = minimax_alpha_beta_ordered(board, player, max_depth=max_depth)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Depth-limited search: {elapsed:.3f}s, {nodes_explored:,} nodes\")\n",
    "    return action\n",
    "\n",
    "# Test\n",
    "print(\"\\nTesting smart player on empty board:\")\n",
    "empty = empty_board()\n",
    "action = smart_minimax_player(empty, 1)\n",
    "print(f\"Chose: {action}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let the Minimax Search agent play a random agent on a small board. Analyze wins, losses and draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Playing Minimax vs Random Agent on 5×5 board\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def minimax_agent_limited(board, player):\n",
    "    \"\"\"Minimax agent with depth limit for practical play.\"\"\"\n",
    "    opening = get_opening_move(board)\n",
    "    if opening:\n",
    "        return opening\n",
    "    _, action = minimax_alpha_beta_ordered(board, player, max_depth=5)\n",
    "    return action\n",
    "\n",
    "# Play games\n",
    "num_games = 20\n",
    "results = {1: 0, -1: 0, 0: 0}\n",
    "\n",
    "print(f\"Playing {num_games} games...\\n\")\n",
    "\n",
    "for i in range(num_games):\n",
    "    if i < num_games // 2:\n",
    "        # Minimax plays first\n",
    "        winner = play_game(minimax_agent_limited, random_player, board_shape=(5, 5))\n",
    "        print(f\"Game {i+1}: Minimax (Player 1) vs Random - Winner: {winner}\")\n",
    "    else:\n",
    "        # Random plays first\n",
    "        winner = play_game(random_player, minimax_agent_limited, board_shape=(5, 5))\n",
    "        # Flip winner for counting (we count from Minimax perspective)\n",
    "        winner = -winner if winner != 0 else 0\n",
    "        print(f\"Game {i+1}: Random vs Minimax (Player 2) - Winner: {winner}\")\n",
    "    \n",
    "    results[winner] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Final Results (from Minimax perspective):\")\n",
    "print(f\"Minimax wins: {results[1]} ({results[1]/num_games*100:.1f}%)\")\n",
    "print(f\"Random wins: {results[-1]} ({results[-1]/num_games*100:.1f}%)\")\n",
    "print(f\"Draws: {results[0]} ({results[0]/num_games*100:.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n**Analysis:**\")\n",
    "print(\"Minimax agent should win significantly more often than random agent,\")\n",
    "print(\"demonstrating the effectiveness of strategic lookahead vs random play.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Heuristic Alpha-Beta Tree Search [3 points] \n",
    "\n",
    "### Heuristic evaluation function\n",
    "\n",
    "Define and implement a heuristic evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_window(window, player):\n",
    "    \"\"\"\n",
    "    Evaluate a window of 4 positions for a player.\n",
    "    Returns a score based on potential to form 4-in-a-row.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    opponent = -player\n",
    "    \n",
    "    player_count = np.sum(window == player)\n",
    "    opponent_count = np.sum(window == opponent)\n",
    "    empty_count = np.sum(window == 0)\n",
    "    \n",
    "    # Scoring system\n",
    "    if player_count == 4:\n",
    "        score += 100  # Win\n",
    "    elif player_count == 3 and empty_count == 1:\n",
    "        score += 5   # Strong position\n",
    "    elif player_count == 2 and empty_count == 2:\n",
    "        score += 2   # Potential\n",
    "    \n",
    "    # Penalize opponent's positions\n",
    "    if opponent_count == 3 and empty_count == 1:\n",
    "        score -= 4  # Must block\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def heuristic_eval(board, player):\n",
    "    \"\"\"\n",
    "    Heuristic evaluation function for non-terminal states.\n",
    "    Evaluates the board from the perspective of the given player.\n",
    "    \n",
    "    Features considered:\n",
    "    - Center column control (strategic advantage)\n",
    "    - Potential winning sequences (horizontal, vertical, diagonal)\n",
    "    - Blocking opponent's threats\n",
    "    \"\"\"\n",
    "    # Check for terminal state first\n",
    "    winner = check_winner(board)\n",
    "    if winner == player:\n",
    "        return 1000\n",
    "    elif winner == -player:\n",
    "        return -1000\n",
    "    \n",
    "    score = 0\n",
    "    rows, cols = board.shape\n",
    "    \n",
    "    # Center column preference\n",
    "    center_col = cols // 2\n",
    "    center_count = np.sum(board[:, center_col] == player)\n",
    "    score += center_count * 3\n",
    "    \n",
    "    # Evaluate horizontal windows\n",
    "    for row in range(rows):\n",
    "        for col in range(cols - 3):\n",
    "            window = board[row, col:col+4]\n",
    "            score += evaluate_window(window, player)\n",
    "    \n",
    "    # Evaluate vertical windows\n",
    "    for col in range(cols):\n",
    "        for row in range(rows - 3):\n",
    "            window = board[row:row+4, col]\n",
    "            score += evaluate_window(window, player)\n",
    "    \n",
    "    # Evaluate positive diagonal windows\n",
    "    for row in range(rows - 3):\n",
    "        for col in range(cols - 3):\n",
    "            window = np.array([board[row+i, col+i] for i in range(4)])\n",
    "            score += evaluate_window(window, player)\n",
    "    \n",
    "    # Evaluate negative diagonal windows\n",
    "    for row in range(3, rows):\n",
    "        for col in range(cols - 3):\n",
    "            window = np.array([board[row-i, col+i] for i in range(4)])\n",
    "            score += evaluate_window(window, player)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# Test heuristic evaluation\n",
    "print(\"Testing Heuristic Evaluation Function\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test board 1: Player 1 has advantage\n",
    "test_board1 = empty_board()\n",
    "test_board1[5, 2] = 1\n",
    "test_board1[5, 3] = 1\n",
    "test_board1[5, 4] = 1\n",
    "test_board1[4, 3] = -1\n",
    "\n",
    "print(\"Test Board 1:\")\n",
    "visualize(test_board1)\n",
    "score = heuristic_eval(test_board1, 1)\n",
    "print(f\"Heuristic score for Player 1: {score}\")\n",
    "\n",
    "# Test board 2: Balanced position\n",
    "test_board2 = empty_board()\n",
    "test_board2[5, 2] = 1\n",
    "test_board2[5, 3] = -1\n",
    "test_board2[5, 4] = 1\n",
    "test_board2[4, 2] = -1\n",
    "\n",
    "print(\"\\nTest Board 2:\")\n",
    "visualize(test_board2)\n",
    "score = heuristic_eval(test_board2, 1)\n",
    "print(f\"Heuristic score for Player 1: {score}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting off search \n",
    "\n",
    "Modify your Minimax Search with Alpha-Beta Pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_heuristic(board, player, alpha=-math.inf, beta=math.inf, depth=0, max_depth=4):\n",
    "    \"\"\"\n",
    "    Minimax with alpha-beta pruning and heuristic cutoff.\n",
    "    \"\"\"\n",
    "    global nodes_explored\n",
    "    nodes_explored += 1\n",
    "    \n",
    "    # Terminal test\n",
    "    if terminal(board):\n",
    "        winner = check_winner(board)\n",
    "        if winner == player:\n",
    "            return 1000, None\n",
    "        elif winner == -player:\n",
    "            return -1000, None\n",
    "        return 0, None\n",
    "    \n",
    "    # Depth cutoff - use heuristic evaluation\n",
    "    if depth >= max_depth:\n",
    "        return heuristic_eval(board, player), None\n",
    "    \n",
    "    valid_actions = actions(board)\n",
    "    if not valid_actions:\n",
    "        return 0, None\n",
    "    \n",
    "    # Order moves for better pruning\n",
    "    valid_actions = order_moves_simple(board, valid_actions, player)\n",
    "    \n",
    "    best_action = valid_actions[0]\n",
    "    \n",
    "    if player == 1:  # Maximizing\n",
    "        value = -math.inf\n",
    "        for action in valid_actions:\n",
    "            new_board = result(board, action, player)\n",
    "            new_value, _ = minimax_heuristic(new_board, -player, alpha, beta, depth+1, max_depth)\n",
    "            \n",
    "            if new_value > value:\n",
    "                value = new_value\n",
    "                best_action = action\n",
    "            \n",
    "            alpha = max(alpha, value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        \n",
    "        return value, best_action\n",
    "    \n",
    "    else:  # Minimizing\n",
    "        value = math.inf\n",
    "        for action in valid_actions:\n",
    "            new_board = result(board, action, player)\n",
    "            new_value, _ = minimax_heuristic(new_board, -player, alpha, beta, depth+1, max_depth)\n",
    "            \n",
    "            if new_value < value:\n",
    "                value = new_value\n",
    "                best_action = action\n",
    "            \n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        \n",
    "        return value, best_action\n",
    "\n",
    "\n",
    "def heuristic_player(board, player, depth=4):\n",
    "    \"\"\"\n",
    "    Agent using heuristic minimax search with specified depth.\n",
    "    \"\"\"\n",
    "    global nodes_explored\n",
    "    nodes_explored = 0\n",
    "    start = time.time()\n",
    "    _, action = minimax_heuristic(board, player, max_depth=depth)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Depth {depth}: {elapsed:.4f}s, {nodes_explored:,} nodes\")\n",
    "    return action\n",
    "\n",
    "\n",
    "# Test with different cutoff depths\n",
    "print(\"Testing Heuristic Minimax with different depths\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_board = empty_board(shape=(6, 7))\n",
    "test_board[5, 3] = 1\n",
    "test_board[5, 2] = -1\n",
    "test_board[4, 3] = 1\n",
    "\n",
    "visualize(test_board)\n",
    "\n",
    "for depth in [2, 4, 6, 8]:\n",
    "    print(f\"\\nDepth {depth}:\")\n",
    "    action = heuristic_player(test_board, 1, depth=depth)\n",
    "    print(f\"Chosen action: {action}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the same manually created boards as above to check if the agent spots winning opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing heuristic agent on manually created boards\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test Board 1: Can win in one move\n",
    "print(\"\\nTest 1: Winning opportunity (vertical)\")\n",
    "board1 = empty_board()\n",
    "board1[5, 3] = 1\n",
    "board1[4, 3] = 1\n",
    "board1[3, 3] = 1\n",
    "board1[5, 2] = -1\n",
    "board1[4, 2] = -1\n",
    "visualize(board1)\n",
    "action = heuristic_player(board1, 1, depth=4)\n",
    "print(f\"Action: {action} - Should win at column 3 by dropping at row 2\")\n",
    "\n",
    "# Test Board 2: Must block opponent\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 2: Must block opponent's winning move\")\n",
    "board2 = empty_board()\n",
    "board2[5, 0] = -1\n",
    "board2[5, 1] = -1\n",
    "board2[5, 2] = -1\n",
    "board2[4, 0] = 1\n",
    "visualize(board2)\n",
    "action = heuristic_player(board2, 1, depth=4)\n",
    "print(f\"Action: {action} - Should block at column 3\")\n",
    "\n",
    "# Test Board 3: Strategic position\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 3: Strategic center control\")\n",
    "board3 = empty_board()\n",
    "board3[5, 3] = 1\n",
    "board3[5, 2] = -1\n",
    "board3[5, 4] = -1\n",
    "visualize(board3)\n",
    "action = heuristic_player(board3, 1, depth=6)\n",
    "print(f\"Action: {action}\")\n",
    "\n",
    "# Test Board 4: Complex position\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 4: Complex mid-game\")\n",
    "board4 = empty_board()\n",
    "board4[5, 2] = 1\n",
    "board4[5, 3] = -1\n",
    "board4[5, 4] = 1\n",
    "board4[4, 3] = 1\n",
    "board4[4, 4] = -1\n",
    "visualize(board4)\n",
    "action = heuristic_player(board4, 1, depth=6)\n",
    "print(f\"Action: {action}\")\n",
    "\n",
    "# Test Board 5: Diagonal threat\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 5: Diagonal opportunity\")\n",
    "board5 = empty_board()\n",
    "board5[5, 1] = 1\n",
    "board5[5, 2] = -1\n",
    "board5[4, 2] = 1\n",
    "board5[5, 3] = -1\n",
    "board5[4, 3] = 1\n",
    "visualize(board5)\n",
    "action = heuristic_player(board5, 1, depth=6)\n",
    "print(f\"Action: {action}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance: Board size vs computation time (with heuristic)\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for cols in range(4, 9):\n",
    "    board = empty_board(shape=(6, cols))\n",
    "    \n",
    "    # Add some initial moves\n",
    "    for i in range(min(3, cols)):\n",
    "        board[5, i] = 1 if i % 2 == 0 else -1\n",
    "    \n",
    "    print(f\"\\nBoard size: 6×{cols}\")\n",
    "    \n",
    "    for depth in [4, 6, 8]:\n",
    "        nodes_explored = 0\n",
    "        start = time.time()\n",
    "        _, action = minimax_heuristic(board, 1, max_depth=depth)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"  Depth {depth}: {elapsed:.4f}s, {nodes_explored:,} nodes\")\n",
    "        \n",
    "        if elapsed > 5:  # Skip deeper searches if taking too long\n",
    "            print(f\"  (Skipping deeper searches for this size)\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- With heuristic evaluation, we can handle standard 6×7 boards\")\n",
    "print(\"- Performance degrades gracefully as depth increases\")\n",
    "print(\"- Depth 6-8 provides good play while remaining computationally feasible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let two heuristic search agents (different cutoff depth, different heuristic evaluation function) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Competition: Heuristic agents with different depths\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def heuristic_player_depth4(board, player):\n",
    "    \"\"\"Heuristic agent with depth 4.\"\"\"\n",
    "    _, action = minimax_heuristic(board, player, max_depth=4)\n",
    "    return action\n",
    "\n",
    "def heuristic_player_depth6(board, player):\n",
    "    \"\"\"Heuristic agent with depth 6.\"\"\"\n",
    "    _, action = minimax_heuristic(board, player, max_depth=6)\n",
    "    return action\n",
    "\n",
    "def heuristic_player_depth8(board, player):\n",
    "    \"\"\"Heuristic agent with depth 8.\"\"\"\n",
    "    _, action = minimax_heuristic(board, player, max_depth=8)\n",
    "    return action\n",
    "\n",
    "\n",
    "# Match 1: Depth 4 vs Depth 6\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Match 1: Depth-4 (Player 1) vs Depth-6 (Player 2)\")\n",
    "print(\"Board: 6×7\")\n",
    "\n",
    "start_time = time.time()\n",
    "winner = play_game(heuristic_player_depth4, heuristic_player_depth6, \n",
    "                   board_shape=(6, 7), verbose=False)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "if winner == 1:\n",
    "    print(f\"Result: Depth-4 wins! (Time: {elapsed:.2f}s)\")\n",
    "elif winner == -1:\n",
    "    print(f\"Result: Depth-6 wins! (Time: {elapsed:.2f}s)\")\n",
    "else:\n",
    "    print(f\"Result: Draw (Time: {elapsed:.2f}s)\")\n",
    "\n",
    "\n",
    "# Match 2: Depth 4 vs Depth 8\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Match 2: Depth-4 (Player 1) vs Depth-8 (Player 2)\")\n",
    "print(\"Board: 6×7\")\n",
    "\n",
    "start_time = time.time()\n",
    "winner = play_game(heuristic_player_depth4, heuristic_player_depth8, \n",
    "                   board_shape=(6, 7), verbose=False)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "if winner == 1:\n",
    "    print(f\"Result: Depth-4 wins! (Time: {elapsed:.2f}s)\")\n",
    "elif winner == -1:\n",
    "    print(f\"Result: Depth-8 wins! (Time: {elapsed:.2f}s)\")\n",
    "else:\n",
    "    print(f\"Result: Draw (Time: {elapsed:.2f}s)\")\n",
    "\n",
    "\n",
    "# Match 3: Depth 6 vs Depth 8\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Match 3: Depth-6 (Player 1) vs Depth-8 (Player 2)\")\n",
    "print(\"Board: 6×7\")\n",
    "\n",
    "start_time = time.time()\n",
    "winner = play_game(heuristic_player_depth6, heuristic_player_depth8, \n",
    "                   board_shape=(6, 7), verbose=False)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "if winner == 1:\n",
    "    print(f\"Result: Depth-6 wins! (Time: {elapsed:.2f}s)\")\n",
    "elif winner == -1:\n",
    "    print(f\"Result: Depth-8 wins! (Time: {elapsed:.2f}s)\")\n",
    "else:\n",
    "    print(f\"Result: Draw (Time: {elapsed:.2f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n**Analysis:**\")\n",
    "print(\"- Deeper search generally leads to better play\")\n",
    "print(\"- However, the improvement diminishes with increasing depth\")\n",
    "print(\"- Depth 6-8 appears to be a good balance between performance and quality\")\n",
    "print(\"- The heuristic evaluation function quality matters as much as search depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament task [+ 1 to 5 bonus point will be assigned separately]\n",
    "\n",
    "Find another student and let your best agent play against the other student's best player. You are allowed to use any improvements you like as long as you code it yourself. We will set up a class tournament on Canvas. This tournament will continue after the submission deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduate student advanced task: Pure Monte Carlo Search and Best First Move [1 point]\n",
    "\n",
    "__Undergraduate students:__ This is a bonus task you can attempt if you like [+1 Bonus point].\n",
    "\n",
    "### Pure Monte Carlo Search\n",
    "\n",
    "Implement Pure Monte Carlo Search (see [tic-tac-toe-example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_pure_monte_carlo_search.ipynb)) and investigate how this search performs on the test boards that you have used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def monte_carlo_playout(board, player):\n",
    "    \"\"\"\n",
    "    Perform a random playout from the current board state.\n",
    "    Returns the winner (1, -1, or 0).\n",
    "    \"\"\"\n",
    "    sim_board = copy.deepcopy(board)\n",
    "    current_player = player\n",
    "    moves = 0\n",
    "    max_moves = 100  # Prevent infinite loops\n",
    "    \n",
    "    while not terminal(sim_board) and moves < max_moves:\n",
    "        valid_actions = actions(sim_board)\n",
    "        if not valid_actions:\n",
    "            break\n",
    "        \n",
    "        # Choose random action\n",
    "        action = random.choice(valid_actions)\n",
    "        sim_board = result(sim_board, action, current_player)\n",
    "        current_player = -current_player\n",
    "        moves += 1\n",
    "    \n",
    "    return check_winner(sim_board)\n",
    "\n",
    "\n",
    "def pure_monte_carlo_search(board, player, num_simulations=1000):\n",
    "    \"\"\"\n",
    "    Pure Monte Carlo Search: Try each possible action and run random playouts.\n",
    "    Choose the action with the best win rate.\n",
    "    \"\"\"\n",
    "    valid_actions = actions(board)\n",
    "    if not valid_actions:\n",
    "        return None\n",
    "    \n",
    "    action_scores = {}\n",
    "    \n",
    "    for action in valid_actions:\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "        \n",
    "        # Run simulations for this action\n",
    "        new_board = result(board, action, player)\n",
    "        \n",
    "        for _ in range(num_simulations):\n",
    "            winner = monte_carlo_playout(new_board, -player)\n",
    "            \n",
    "            if winner == player:\n",
    "                wins += 1\n",
    "            elif winner == -player:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "        \n",
    "        # Score: wins - losses\n",
    "        score = wins - losses\n",
    "        action_scores[action] = {\n",
    "            'score': score,\n",
    "            'wins': wins,\n",
    "            'losses': losses,\n",
    "            'draws': draws\n",
    "        }\n",
    "    \n",
    "    # Choose action with best score\n",
    "    best_action = max(action_scores.items(), key=lambda x: x[1]['score'])[0]\n",
    "    \n",
    "    return best_action, action_scores\n",
    "\n",
    "\n",
    "def monte_carlo_player(board, player, num_sims=500):\n",
    "    \"\"\"\n",
    "    Agent using Pure Monte Carlo Search.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    result_tuple = pure_monte_carlo_search(board, player, num_simulations=num_sims)\n",
    "    \n",
    "    if result_tuple is None:\n",
    "        return None\n",
    "    \n",
    "    best_action, scores = result_tuple\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Monte Carlo ({num_sims} sims/action): {elapsed:.2f}s\")\n",
    "    print(f\"Best action: {best_action}, Score: {scores[best_action]['score']}\")\n",
    "    \n",
    "    return best_action\n",
    "\n",
    "\n",
    "# Test Monte Carlo on the same boards\n",
    "print(\"Testing Pure Monte Carlo Search\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTest 1: Obvious winning move\")\n",
    "board1 = empty_board(shape=(6, 7))\n",
    "board1[5, 3] = 1\n",
    "board1[4, 3] = 1\n",
    "board1[3, 3] = 1\n",
    "board1[5, 2] = -1\n",
    "board1[4, 2] = -1\n",
    "visualize(board1)\n",
    "action = monte_carlo_player(board1, 1, num_sims=200)\n",
    "print(f\"Chosen: {action}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTest 2: Must block opponent\")\n",
    "board2 = empty_board(shape=(6, 7))\n",
    "board2[5, 0] = -1\n",
    "board2[5, 1] = -1\n",
    "board2[5, 2] = -1\n",
    "board2[4, 0] = 1\n",
    "visualize(board2)\n",
    "action = monte_carlo_player(board2, 1, num_sims=200)\n",
    "print(f\"Chosen: {action}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n**Observations:**\")\n",
    "print(\"- Monte Carlo can find good moves without explicit search tree\")\n",
    "print(\"- Performance depends on number of simulations\")\n",
    "print(\"- Works well for tactical positions (immediate threats)\")\n",
    "print(\"- May miss complex strategic patterns that require deeper lookahead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best First Move\n",
    "\n",
    "How would you determine what the best first move for a standard board ($6 \\times 7$) is? You can use Pure Monte Carlo Search or any algorithms that you have implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Determining the Best First Move for Standard 6×7 Board\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "**Approach:**\n",
    "Use Monte Carlo simulations to evaluate each possible first move.\n",
    "\n",
    "**Method:**\n",
    "1. For each of the 7 possible first moves\n",
    "2. Run many random playouts\n",
    "3. Record win/loss/draw statistics\n",
    "4. Choose the move with the best winning percentage\n",
    "\n",
    "**Why this works:**\n",
    "- Don't need to evaluate to terminal states\n",
    "- Statistical approach averages over many possible game continuations\n",
    "- Computationally feasible even for empty board\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRunning analysis...\\n\")\n",
    "\n",
    "empty = empty_board(shape=(6, 7))\n",
    "first_move_results = {}\n",
    "\n",
    "# Test each column\n",
    "for col in range(7):\n",
    "    print(f\"Analyzing first move in column {col}...\")\n",
    "    action = ('drop', col)\n",
    "    \n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    num_sims = 500  # Simulations per first move\n",
    "    \n",
    "    new_board = result(empty, action, 1)\n",
    "    \n",
    "    for i in range(num_sims):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Simulation {i + 1}/{num_sims}\")\n",
    "        winner = monte_carlo_playout(new_board, -1)\n",
    "        \n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "        elif winner == -1:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    win_rate = wins / num_sims\n",
    "    first_move_results[col] = {\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'draws': draws,\n",
    "        'win_rate': win_rate\n",
    "    }\n",
    "    \n",
    "    print(f\"  Column {col}: {wins}W-{losses}L-{draws}D (Win rate: {win_rate:.1%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for col in range(7):\n",
    "    stats = first_move_results[col]\n",
    "    print(f\"Column {col}: Win rate = {stats['win_rate']:.1%} \"\n",
    "          f\"({stats['wins']}W-{stats['losses']}L-{stats['draws']}D)\")\n",
    "\n",
    "best_col = max(first_move_results.items(), key=lambda x: x[1]['win_rate'])[0]\n",
    "print(f\"\\n**Best first move: Column {best_col}**\")\n",
    "print(f\"Win rate: {first_move_results[best_col]['win_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n**Expected Result:**\")\n",
    "print(\"Column 3 (center) should have the highest win rate, which is\")\n",
    "print(\"consistent with Connect 4 theory. The center column provides:\")\n",
    "print(\"- Maximum flexibility for future moves\")\n",
    "print(\"- Control of the most valuable board position\")\n",
    "print(\"- Equal opportunities for horizontal, vertical, and diagonal formations\")\n",
    "print(\"\\nThis validates both the Monte Carlo approach and our game implementation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
